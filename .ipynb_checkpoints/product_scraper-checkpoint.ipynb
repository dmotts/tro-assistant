{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "from config import setup_logging\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from Product import Product\n",
    "from app import add_to_db\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging = setup_logging()\n",
    "\n",
    "# Set file paths for product URLs and selectors\n",
    "PRODUCT_URLS_FILE = 'products/product-urls.txt'\n",
    "SELECTORS_FILE = 'products/product-selectors.txt'\n",
    "\n",
    "# Set header information\n",
    "user_agent = 'Mozilla/5.0 (Linux; Android 11; 100011886A Build/RP1A.200720.011) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.69 Safari/537.36'\n",
    "sec_ch_ua = '\"Google Chrome\";v=\"104\", \" Not;A Brand\";v=\"105\", \"Chromium\";v=\"104\"'\n",
    "referer = 'https://www.google.com'\n",
    "cache_control = 'no-cache'\n",
    "content_type = 'application/json'\n",
    "\n",
    "# Set up Firefox WebDriver with headless mode\n",
    "FIREFOX_LOCATION = os.environ.get('FIREFOX_LOCATION', os.getenv('FIREFOX_LOCATION'))\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "def save_urls_to_txt(urls, base_url):\n",
    "    # Create the 'products' directory if it doesn't exist\n",
    "    if not os.path.exists(\"products\"):\n",
    "        os.makedirs(\"products\")\n",
    "\n",
    "    file_name = os.path.join(\"products\", \"product-urls.txt\")  # Fallback filename if parsing fails\n",
    "\n",
    "    # Create a set to store unique URLs\n",
    "    unique_urls = set()\n",
    "\n",
    "    # Check if the file already exists and read existing URLs into the set\n",
    "    try:\n",
    "        if os.path.isfile(file_name):\n",
    "            with open(file_name, 'r') as file:\n",
    "                existing_urls = file.read().splitlines()\n",
    "                unique_urls.update(existing_urls)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Add new unique URLs to the set\n",
    "    unique_urls.update(urls)\n",
    "\n",
    "    # Write the unique URLs to the file\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.writelines(f\"{url}\\n\" for url in unique_urls)\n",
    "\n",
    "    logging.info(f'Product URLs saved to {file_name}')\n",
    "\n",
    "def extract_product_urls(base_url, selector, next_button_selector=None, max_pages=None, max_products=None):\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    try:\n",
    "        urls = []\n",
    "        page_count = 0\n",
    "        current_url = base_url\n",
    "\n",
    "        while current_url and (max_pages is None or page_count < max_pages):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                driver.get(current_url)\n",
    "\n",
    "                # Calculate dynamic sleep time based on the website's response time\n",
    "                response_time = time.time() - start_time\n",
    "                sleep_time = max(1, min(10, response_time * 2))  # Dynamic sleep time (between 1 and 10 seconds)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                urls.extend(element.get_attribute(\"href\") for element in elements)\n",
    "\n",
    "                if max_products and len(urls) >= max_products:\n",
    "                    urls = urls[:max_products]  # Truncate the list to the specified maximum\n",
    "\n",
    "                if len(urls) >= max_products:\n",
    "                    break  # Stop when reaching the maximum number of product URLs\n",
    "\n",
    "                if next_button_selector:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, next_button_selector)\n",
    "                    current_url = next_button.get_attribute(\"href\")\n",
    "                else:\n",
    "                    current_url = None\n",
    "\n",
    "                page_count += 1\n",
    "            except NoSuchElementException:\n",
    "                logging.info(f\"Selector not found on page: {current_url}\")\n",
    "                break\n",
    "            except TimeoutException:\n",
    "                logging.info(f\"Page load timed out for URL: {current_url}\")\n",
    "                break\n",
    "\n",
    "        save_urls_to_txt(urls, base_url)  # Save the URLs to a TXT file\n",
    "\n",
    "        return urls\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def scrape_single_product(product_url, selectors, download_directory='docs'):\n",
    "    \"\"\"\n",
    "    Scrapes product information from a given URL and downloads datasheets if available.\n",
    "\n",
    "    Args:\n",
    "        product_url (str): The URL of the product to scrape.\n",
    "        selectors (dict): A dictionary of selectors for different attributes of the product.\n",
    "        download_directory (str, optional): The directory to save downloaded datasheets. Defaults to 'docs'.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing scraped product information.\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    options.add_argument('--headless')\n",
    "\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    product = Product(driver, product_url, **selectors)\n",
    "    product.scrape()\n",
    "\n",
    "    product_info = {}\n",
    "    for key, selector in selectors.items():\n",
    "        try:\n",
    "            if key == \"image\":\n",
    "                value = product.get_image_url(selector)\n",
    "            elif key == \"brand\":\n",
    "                value = product.get_brand(selector)\n",
    "            elif key == \"datasheets\":\n",
    "                datasheets = product.get_datasheet_links(selector)\n",
    "                value = []\n",
    "                for datasheet_name, datasheet_url in datasheets:\n",
    "                    # Modify each datasheet link to be a clickable name\n",
    "                    datasheet_link = f\"[{datasheet_name}]({datasheet_url})\"\n",
    "                    value.append(datasheet_link)\n",
    "                    # Download the datasheet using the download_datasheet method\n",
    "                    product.download_datasheet(datasheet_url, download_directory)\n",
    "            else:\n",
    "                value = product.get_attribute(key)\n",
    "                # Modify the 'name' key to be a clickable link to the product URL\n",
    "                if key == \"name\":\n",
    "                    value = f\"[{value}]({product_url})\"\n",
    "            product_info[key] = value\n",
    "            logging.info(f\"{key.capitalize()}: {value}\")\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Failed to retrieve {key} - {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return product_info\n",
    "\n",
    "def scrape_multiple_products(urls_and_selectors):\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        results = list(executor.map(lambda x: scrape_single_product(*x), urls_and_selectors))\n",
    "\n",
    "    return results\n",
    "\n",
    "def read_selectors_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            selectors = json.load(file)\n",
    "        return selectors\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return {}\n",
    "\n",
    "def scrape_products():\n",
    "    # Read product URLs from a file\n",
    "    with open(PRODUCT_URLS_FILE, 'r') as file:\n",
    "        product_urls = [line.strip() for line in file]\n",
    "\n",
    "    # Read selectors from a file\n",
    "    selectors = read_selectors_from_file(SELECTORS_FILE)\n",
    "\n",
    "    if not selectors:\n",
    "        print(\"No selectors found. Please check the product_selectors.txt file.\")\n",
    "        return\n",
    "\n",
    "    urls_and_selectors = [(url, selectors) for url in product_urls]\n",
    "\n",
    "    # Scrape multiple products\n",
    "    product_info_list = scrape_multiple_products(urls_and_selectors)\n",
    "\n",
    "    # Create the 'products' directory if it doesn't exist\n",
    "    os.makedirs(\"products\", exist_ok=True)\n",
    "\n",
    "    # Save product information to a markdown file in the 'products' directory\n",
    "    base_url = product_urls[0].split('//')[1].split('/')[0]  # Extract the domain name\n",
    "    markdown_file_name = f\"products/products-info.md\"\n",
    "\n",
    "    with open(markdown_file_name, 'w') as md_file:\n",
    "        for product_info in product_info_list:\n",
    "            md_file.write(\"## Product Information\\n\\n\")\n",
    "            for key, value in product_info.items():\n",
    "                if key == \"image\" and value:  # Check if the key is \"image\" and there are image URLs\n",
    "                    md_file.write(f\"**{key.capitalize()}**: \\n\")  # Start the line with the image key\n",
    "                    for img_url in value:\n",
    "                        md_file.write(f\"![Image]({img_url})\\n\")  # Add the image tag\n",
    "                else:\n",
    "                    md_file.write(f\"**{key.capitalize()}**: {value}\\n\\n\")  # Regular text for other keys\n",
    "\n",
    "    # Find the product URLs file with the timestamp suffix in the 'uploaded' directory\n",
    "    uploaded_files = [f for f in os.listdir(\"uploaded\") if f.startswith(\"product-urls-processed_\")]\n",
    "    if uploaded_files:\n",
    "        uploaded_file = os.path.join(\"uploaded\", uploaded_files[0])\n",
    "\n",
    "        # Amend the contents of the product URLs file\n",
    "        with open(uploaded_file, 'a') as uploaded_content:\n",
    "            for url in product_urls:\n",
    "                uploaded_content.write(f\"{url}\\n\")\n",
    "\n",
    "        # Rename the file with the current time and date suffix\n",
    "        current_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        renamed_file_name = f\"uploaded/product-urls-processed_{current_datetime}.txt\"\n",
    "        os.replace(uploaded_file, renamed_file_name)\n",
    "\n",
    "    return product_info_list\n",
    "\n",
    "def print_product_list(product_list):\n",
    "    # Print or process the list of product information as needed\n",
    "    for product_info in product_list:\n",
    "        for key, value in product_info.items():\n",
    "            logging.info(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_url = 'https://www.tro.com.au/enclosures/wall-mount-enclosures/steel-wall-mount-enclosures'\n",
    "    selector = \"a.facets-item-cell-grid-title\"\n",
    "    next_button_selector = \".global-views-pagination-next > a\"\n",
    "    max_pages = 1\n",
    "    max_products = 5\n",
    "\n",
    "    urls = [\n",
    "        'https://www.tro.com.au/tools/slotted-duct-cutters'\n",
    "    ]\n",
    "\n",
    "    for base_url in urls:\n",
    "        product_urls = extract_product_urls(base_url, selector, next_button_selector, max_pages=max_pages, max_products=max_products)\n",
    "        logging.info(product_urls)\n",
    "\n",
    "    product_info_list = scrape_products()\n",
    "    print_product_list(product_info_list)\n",
    "   \n",
    "    #add_to_db()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
